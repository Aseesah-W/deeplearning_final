**‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏á‡∏≤‡∏ô**

Deep Learning-Based Nail Disease Detection using CNN with Spatial Attention

**‡∏ó‡∏≥‡πÑ‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à ‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å**

‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡πá‡∏ö‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡πà‡∏á‡∏ö‡∏≠‡∏Å‡∏ñ‡∏∂‡∏á‡∏†‡∏≤‡∏ß‡∏∞‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏£‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô ‡πÇ‡∏£‡∏Ñ‡πÇ‡∏•‡∏´‡∏¥‡∏ï‡∏à‡∏≤‡∏á ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏£‡∏Ñ‡∏´‡∏±‡∏ß‡πÉ‡∏à ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡πÄ‡∏•‡πá‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏≤‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏°‡∏±‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ú‡∏¥‡∏ß‡∏´‡∏ô‡∏±‡∏á ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Deep Learning ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Convolutional Neural Network (CNN) ‡∏à‡∏∂‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏£‡∏Ñ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡πÄ‡∏•‡πá‡∏ö‡πÑ‡∏î‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á

‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ AI ‡∏î‡πâ‡∏≤‡∏ô‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ß‡∏¥‡∏ó‡∏±‡∏®‡∏ô‡πå (Computer Vision) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏™‡∏°‡∏≤‡∏£‡πå‡∏ï‡πÇ‡∏ü‡∏ô‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡πÑ‡∏î‡πâ

**‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Deep Learning ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ‡∏≠‡∏∑‡πà‡∏ô**

| **‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£** | **‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô** | **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ** | **‡∏Ç‡πâ‡∏≠‡∏î‡πâ‡∏≠‡∏¢** |
| --- | --- | --- | --- |
| Traditional Image Processing | ‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ threshold, edge detection, color segmentation | ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢, ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ô‡πâ‡∏≠‡∏¢ | ‡πÑ‡∏°‡πà‡∏ó‡∏ô‡∏ï‡πà‡∏≠‡πÅ‡∏™‡∏á ‡∏™‡∏µ ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡πá‡∏ö |
| Classical Machine Learning | ‡πÉ‡∏ä‡πâ feature ‡πÄ‡∏ä‡πà‡∏ô texture, color ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ä‡πâ SVM ‡∏´‡∏£‡∏∑‡∏≠ Random Forest | ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô | ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö feature ‡∏î‡πâ‡∏ß‡∏¢‡∏°‡∏∑‡∏≠ (manual feature extraction) |
| Deep Learning (CNN) | ‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ feature ‡πÄ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡∏î‡∏¥‡∏ö | ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á, ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥, ‡∏Ç‡∏¢‡∏≤‡∏¢‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏î‡πâ‡∏î‡∏µ | ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ dataset ‡∏°‡∏≤‡∏Å‡πÅ‡∏•‡∏∞‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏™‡∏π‡∏á |

‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Deep Learning (CNN) ‡∏à‡∏∂‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡πá‡∏ö‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö feature ‡πÄ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏°‡∏î‡∏π‡∏• Attention ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ô‡πâ‡∏ô‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ

**‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• (Model Architecture)**

‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Ñ‡∏∑‡∏≠ ResNet-18 ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô CNN ‡∏ó‡∏µ‡πà‡∏°‡∏µ Residual Block ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ gradient ‡πÑ‡∏´‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÉ‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡∏•‡∏∂‡∏Å ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£ "‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡πÄ‡∏≠‡∏á" ‡∏Ñ‡∏∑‡∏≠ Spatial Attention Module

‡πÇ‡∏°‡πÄ‡∏î‡∏• ResNet18WithAttention ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å convolution ‡∏Ç‡∏ô‡∏≤‡∏î 7√ó7 (3‚Üí64) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ batch normalization ‡πÅ‡∏•‡∏∞ ReLU ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏î spatial dimension ‡∏î‡πâ‡∏ß‡∏¢ max-pooling ‡∏Å‡πà‡∏≠‡∏ô‡∏à‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏™‡πÄ‡∏ï‡∏à‡∏Ç‡∏≠‡∏á residual blocks ‡πÅ‡∏ö‡∏ö ResNet-18 ‡∏ã‡∏∂‡πà‡∏á‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ 4 ‡∏Å‡∏•‡∏∏‡πà‡∏° (‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏°‡∏µ 2 BasicBlocks) ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà (feature maps) ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô 64 ‚Üí 128 ‚Üí 256 ‚Üí 512 ‡πÇ‡∏î‡∏¢‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ç‡∏ô‡∏≤‡∏î spatial ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏≥‡∏ô‡∏ß‡∏ô channel ‡∏à‡∏∞‡πÉ‡∏ä‡πâ downsample (1√ó1 convolution + BN) ‡πÉ‡∏ô‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á skip ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏ß‡∏Å‡∏£‡∏ß‡∏°‡πÅ‡∏ö‡∏ö residual ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ residual connection ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡πÑ‡∏´‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤ vanishing gradient

‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏î‡πâ feature map ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏ô‡∏≤‡∏î 512√ó7√ó7 ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡∏ô‡∏≥ feature map ‡∏ô‡∏±‡πâ‡∏ô‡πÑ‡∏õ‡∏ú‡πà‡∏≤‡∏ô Spatial Attention ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö channel-wise (max ‡πÅ‡∏•‡∏∞ average pooling) ‡πÅ‡∏•‡πâ‡∏ß‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢ convolution 7√ó7 ‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ sigmoid ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á attention mask ‡∏Ç‡∏ô‡∏≤‡∏î 1√ó7√ó7 ‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏π‡∏ì‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö feature map ‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏£‡∏ß‡∏° (channel-wise multiplication) ‡∏™‡πà‡∏á‡∏ú‡∏•‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ô‡πâ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏≤‡∏á spatial ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ä‡πâ adaptive average pooling ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î spatial dimension ‡πÄ‡∏õ‡πá‡∏ô 1√ó1 ‡∏ï‡πà‡∏≠ channel ‡πÅ‡∏•‡πâ‡∏ß flatten ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß 512 ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤ fully-connected layer (512‚Üí6) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏•‡∏≤‡∏™‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢

**‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°**

**Activation Function:**

‡πÉ‡∏ä‡πâ ReLU ‡πÉ‡∏ô ResNet ‡πÅ‡∏•‡∏∞ Sigmoid ‡πÉ‡∏ô Attention Module ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0-1

**‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÇ‡∏´‡∏ô‡∏î‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÇ‡∏î‡∏¢‡∏™‡∏£‡∏∏‡∏õ:**

| **Layer** | **Input Shape** | **Output Shape** | **Activation** |
| --- | --- | --- | --- |
| Conv1 | (3, 224, 224) | (64, 112, 112) | ReLU |
| ResNet Blocks | (64-512, ‚Ä¶) | (512, 7, 7) | ReLU |
| Spatial Attention | (512, 7, 7) | (1, 7, 7) | Sigmoid |
| FC Layer | (512,) | (6,) | Softmax |

**‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÇ‡∏Ñ‡πâ‡∏î (Code Explanation)**

**Section 1-2:** ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡πÄ‡∏ä‡πà‡∏ô batch size, epochs, image size

\# ===========================

\# 1. Install & Import Packages

\# ===========================

!pip install torch torchvision torchaudio matplotlib seaborn scikit-learn

!pip install kagglehub

import torch

import torch.nn as nn

import torch.optim as optim

from torch.utils.data import DataLoader

from torchvision import datasets, transforms, models

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.metrics import confusion_matrix, accuracy_score

import numpy as np

import os

import kagglehub

path = kagglehub.dataset_download("nikhilgurav21/nail-disease-detection-dataset")

print("üìÅ Dataset downloaded to:", path)

import os

for root, dirs, files in os.walk(path):

&nbsp;   level = root.replace(path, '').count(os.sep)

&nbsp;   indent = ' ' \* 2 \* level

&nbsp;   print(f"{indent}{os.path.basename(root)}/")

&nbsp;   subindent = ' ' \* 2 \* (level + 1)

&nbsp;   for f in files:

&nbsp;       print(f"{subindent}{f}")

\# ===========================

\# 2. Config

\# ===========================

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

BATCH_SIZE = 16

EPOCHS = 10

NUM_CLASSES = 6  # healthy + 5 diseases

IMAGE_SIZE = 224

DATASET_PATH = os.path.join(path, "data")

**Section 3:** ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Augmentation & DataLoader) ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ torchvision.datasets.ImageFolder ‡πÅ‡∏•‡∏∞ transforms ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ normalization, rotation, color jitter ‡∏Ø‡∏•‡∏Ø

\# ===========================

\# 3. Data Augmentation & Loader

\# ===========================

train_transforms = transforms.Compose(\[

&nbsp;   transforms.RandomResizedCrop(IMAGE_SIZE),

&nbsp;   transforms.RandomHorizontalFlip(),

&nbsp;   transforms.RandomRotation(15),

&nbsp;   transforms.ColorJitter(brightness=0.2, contrast=0.2),

&nbsp;   transforms.ToTensor(),

&nbsp;   transforms.Normalize(mean=\[0.485,0.456,0.406\], std=\[0.229,0.224,0.225\])

\])

val_transforms = transforms.Compose(\[

&nbsp;   transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),

&nbsp;   transforms.ToTensor(),

&nbsp;   transforms.Normalize(mean=\[0.485,0.456,0.406\], std=\[0.229,0.224,0.225\])

\])

train_dataset = datasets.ImageFolder(os.path.join(DATASET_PATH, "train"), transform=train_transforms)

val_dataset = datasets.ImageFolder(os.path.join(DATASET_PATH, "validation"), transform=val_transforms)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

class_names = train_dataset.classes

print("Classes:", class_names)

**Section 4:** ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏•‡∏≤‡∏™ SpatialAttention ‡∏ã‡∏∂‡πà‡∏á‡∏£‡∏ß‡∏°‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ä‡πâ convolution 7√ó7 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ô‡πâ‡∏ô‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

\# ===========================

\# 4. Spatial Attention Block

\# ===========================

class SpatialAttention(nn.Module):

&nbsp;   def \__init_\_(self, kernel_size=7):

&nbsp;       super(SpatialAttention, self).\__init_\_()

&nbsp;       self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2)

&nbsp;       self.sigmoid = nn.Sigmoid()

&nbsp;   def forward(self, x):

&nbsp;       avg_out = torch.mean(x, dim=1, keepdim=True)

&nbsp;       max_out,_ = torch.max(x, dim=1, keepdim=True)

&nbsp;       x = torch.cat(\[avg_out, max_out\], dim=1)

&nbsp;       x = self.conv(x)

&nbsp;       return self.sigmoid(x)

**Section 5:** ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ResNet18WithAttention ‡πÇ‡∏î‡∏¢‡∏ô‡∏≥ ResNet18 ‡∏°‡∏≤‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô fully connected ‡πÄ‡∏î‡∏¥‡∏°‡∏≠‡∏≠‡∏Å ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏°‡∏î‡∏π‡∏• Attention

\# ===========================

\# 5. ResNet18 + Attention Model

\# ===========================

class ResNet18WithAttention(nn.Module):

&nbsp;   def \__init_\_(self, num_classes=NUM_CLASSES):

&nbsp;       super().\__init_\_()

&nbsp;       base_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)

&nbsp;       self.features = nn.Sequential(\*list(base_model.children())\[:-2\])  # Remove avgpool & fc

&nbsp;       self.attn = SpatialAttention()

&nbsp;       self.pool = nn.AdaptiveAvgPool2d((1, 1))

&nbsp;       self.fc = nn.Linear(512, num_classes)

&nbsp;   def forward(self, x):

&nbsp;       x = self.features(x)

&nbsp;       attn_map = self.attn(x)

&nbsp;       x = x \* attn_map

&nbsp;       x = self.pool(x)

&nbsp;       x = torch.flatten(x, 1)

&nbsp;       x = self.fc(x)

&nbsp;       return x

model = ResNet18WithAttention().to(DEVICE)

print(model)

**Section 6-7:** ‡∏™‡∏£‡πâ‡∏≤‡∏á loss (CrossEntropyLoss) ‡πÅ‡∏•‡∏∞ optimizer (Adam), ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì accuracy

\# ===========================

\# 6. Loss & Optimizer

\# ===========================

criterion = nn.CrossEntropyLoss()

optimizer = optim.Adam(model.parameters(), lr=1e-4)

\# ===========================

\# 7. Training Loop

\# ===========================

train_losses, val_losses = \[\], \[\]

train_acc, val_acc = \[\], \[\]

for epoch in range(EPOCHS):

&nbsp;   model.train()

&nbsp;   running_loss, correct, total = 0, 0, 0

&nbsp;   for images, labels in train_loader:

&nbsp;       images, labels = images.to(DEVICE), labels.to(DEVICE)

&nbsp;       optimizer.zero_grad()

&nbsp;       outputs = model(images)

&nbsp;       loss = criterion(outputs, labels)

&nbsp;       loss.backward()

&nbsp;       optimizer.step()

&nbsp;       running_loss += loss.item() \* images.size(0)

&nbsp;       \_, predicted = torch.max(outputs, 1)

&nbsp;       total += labels.size(0)

&nbsp;       correct += (predicted == labels).sum().item()

&nbsp;   train_losses.append(running_loss/total)

&nbsp;   train_acc.append(correct/total)

&nbsp;   # Validation

&nbsp;   model.eval()

&nbsp;   val_loss, correct_val, total_val = 0,0,0

&nbsp;   with torch.no_grad():

&nbsp;       for images, labels in val_loader:

&nbsp;           images, labels = images.to(DEVICE), labels.to(DEVICE)

&nbsp;           outputs = model(images)

&nbsp;           loss = criterion(outputs, labels)

&nbsp;           val_loss += loss.item() \* images.size(0)

&nbsp;           \_, predicted = torch.max(outputs, 1)

&nbsp;           total_val += labels.size(0)

&nbsp;           correct_val += (predicted == labels).sum().item()

&nbsp;   val_losses.append(val_loss/total_val)

&nbsp;   val_acc.append(correct_val/total_val)

&nbsp;   print(f"Epoch \[{epoch+1}/{EPOCHS}\] "

&nbsp;         f"Train Loss: {train_losses\[-1\]:.4f}, Train Acc: {train_acc\[-1\]\*100:.2f}% "

&nbsp;         f"Val Loss: {val_losses\[-1\]:.4f}, Val Acc: {val_acc\[-1\]\*100:.2f}%")

&nbsp;       # üîπ Show predictions every 2 epochs

&nbsp;   if (epoch+1) % 2 == 0:

&nbsp;       images, labels = next(iter(val_loader))

&nbsp;       images, labels = images.to(DEVICE), labels.to(DEVICE)

&nbsp;       outputs = model(images)

&nbsp;       \_, preds = torch.max(outputs, 1)

&nbsp;       plt.figure(figsize=(12, 6))

&nbsp;       for i in range(6):

&nbsp;           plt.subplot(2, 3, i+1)

&nbsp;           img = images\[i\].cpu().permute(1, 2, 0).numpy()

&nbsp;           img = (img - img.min()) / (img.max() - img.min())

&nbsp;           plt.imshow(img)

&nbsp;           plt.title(f"Pred: {class_names\[preds\[i\]\]}\\nTrue: {class_names\[labels\[i\]\]}")

&nbsp;           plt.axis('off')

&nbsp;       plt.suptitle(f"Validation samples after epoch {epoch+1}")

&nbsp;       plt.show()

**Section 8:** Plot of Training and Validation Loss & Accuracy

\# ===========================

\# 8. Plot Loss & Accuracy

\# ===========================

plt.figure(figsize=(10,4))

\# --- ‡∏Å‡∏£‡∏≤‡∏ü Loss ---

plt.subplot(1,2,1)

plt.plot(train_losses, label='Train Loss', marker='o')

plt.plot(val_losses, label='Val Loss', marker='o')

plt.title('Training vs Validation Loss')

plt.xlabel('Epoch')

plt.ylabel('Loss')

plt.legend()

\# --- ‡∏Å‡∏£‡∏≤‡∏ü Accuracy ---

plt.subplot(1,2,2)

plt.plot(train_acc, label='Train Accuracy', marker='o')

plt.plot(val_acc, label='Val Accuracy', marker='o')

plt.title('Training vs Validation Accuracy')

plt.xlabel('Epoch')

plt.ylabel('Accuracy')

plt.legend()

plt.show()

**Section 9:** ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢ confusion matrix ‡πÅ‡∏•‡∏∞ visualize attention heatmap ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏ô‡πÉ‡∏à‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡πÄ‡∏•‡πá‡∏ö‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà

\# ===========================

\# 9. Confusion Matrix

\# ===========================

all_preds, all_labels = \[\], \[\]

model.eval()

with torch.no_grad():

&nbsp;   for images, labels in val_loader:

&nbsp;       images = images.to(DEVICE)

&nbsp;       outputs = model(images)

&nbsp;       \_, preds = torch.max(outputs,1)

&nbsp;       all_preds.extend(preds.cpu().numpy())

&nbsp;       all_labels.extend(labels.numpy())

cm = confusion_matrix(all_labels, all_preds)

plt.figure(figsize=(8,6))

sns.heatmap(cm, annot=True, fmt="d", xticklabels=class_names, yticklabels=class_names, cmap="Blues")

plt.xlabel("Predicted")

plt.ylabel("True")

plt.title("Confusion Matrix")

plt.show()

from PIL import Image

def visualize_attention(model, image_path, class_names):

&nbsp;   model.eval()

&nbsp;   image = Image.open(image_path).convert("RGB")

&nbsp;   transform = val_transforms

&nbsp;   img_tensor = transform(image).unsqueeze(0).to(DEVICE)

&nbsp;   with torch.no_grad():

&nbsp;       output = model(img_tensor)

&nbsp;       probs = torch.softmax(output, dim=1)

&nbsp;       conf, pred = torch.max(probs, dim=1)

&nbsp;       # attention visualization (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)

&nbsp;       features = model.features(img_tensor)

&nbsp;       attn_map = model.attn(features)

&nbsp;       attn_map = attn_map.squeeze().cpu().numpy()

&nbsp;       attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min())

&nbsp;   plt.figure(figsize=(10, 5))

&nbsp;   plt.subplot(1, 2, 1)

&nbsp;   plt.imshow(image)

&nbsp;   plt.title(f"Prediction: {class_names\[pred.item()\]} ({conf.item()\*100:.2f}%)")

&nbsp;   plt.subplot(1, 2, 2)

&nbsp;   plt.imshow(image)

&nbsp;   plt.imshow(attn_map, cmap='jet', alpha=0.4)

&nbsp;   plt.title("Attention Heatmap")

&nbsp;   plt.show()

visualize_attention(model, f"{DATASET_PATH}/validation/clubbing/Screen-Shot-2021-10-26-at-12-10-57-PM_png.rf.13d80dc781bd7e9b4d1c2c67ecbacb55.jpg",class_names)

visualize_attention(model, f"{DATASET_PATH}/validation/clubbing/Screen-Shot-2021-10-26-at-12-11-28-PM_png.rf.4f993b88f526fd3ce51dc24eeae5b4d2.jpg",class_names)

visualize_attention(model, f"{DATASET_PATH}/validation/clubbing/Screen-Shot-2021-10-26-at-12-07-03-PM_png.rf.3514795dcb0a1e7bbbd78fd0acc2ac7b.jpg",class_names)

**‡∏•‡∏¥‡∏á‡∏Ñ‡πå‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡πÑ‡∏õ‡∏™‡∏π‡πà‡πÇ‡∏Ñ‡πâ‡∏î**

<https://github.com/Aseesah-W/deeplearning_final/blob/main/Deep_Learning_Based_Nail_Health_Analysis.ipynb>

**Dataset ‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤**

- ‡∏ä‡∏∑‡πà‡∏≠ Dataset: Nail Disease Detection Dataset
- ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤: [Kaggle - nikhilgurav21/nail-disease-detection-dataset](https://www.kaggle.com/datasets/nikhilgurav21/nail-disease-detection-dataset)
- ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡∏∏‡∏î‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡∏à‡∏∏‡∏î‡∏°‡∏∏‡πà‡∏á‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÇ‡∏£‡∏Ñ‡πÄ‡∏•‡πá‡∏ö
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™: 6 ‡∏Ñ‡∏•‡∏≤‡∏™ ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà acral_lentiginous_melanoma, healthy_nail, onychogryphosis, blue_finger, clubbing, pitting
- ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå: JPEG

**‡∏Å‡∏≤‡∏£ Train ‡πÅ‡∏•‡∏∞ Evaluate ‡πÇ‡∏°‡πÄ‡∏î‡∏•**

- Hardware: GPU (Colab CUDA)
- Optimizer: Adam (lr=1e-4)
- Loss Function: CrossEntropyLoss
- Batch Size: 16
- Epochs: 10

Epoch \[1/10\] Train Loss: 0.9951, Train Acc: 62.45% Val Loss: 0.3632, Val Acc: 87.91%

Epoch \[2/10\] Train Loss: 0.6242, Train Acc: 77.03% Val Loss: 0.2816, Val Acc: 87.91%

Epoch \[3/10\] Train Loss: 0.5220, Train Acc: 80.80% Val Loss: 0.2957, Val Acc: 90.11%

Epoch \[4/10\] Train Loss: 0.5048, Train Acc: 82.21% Val Loss: 0.2729, Val Acc: 92.31%

Epoch \[5/10\] Train Loss: 0.4578, Train Acc: 83.39% Val Loss: 0.1491, Val Acc: 95.60%

Epoch \[6/10\] Train Loss: 0.3984, Train Acc: 85.63% Val Loss: 0.2788, Val Acc: 85.71%

Epoch \[7/10\] Train Loss: 0.3735, Train Acc: 86.40% Val Loss: 0.3460, Val Acc: 85.71%

Epoch \[8/10\] Train Loss: 0.3787, Train Acc: 86.08% Val Loss: 0.2438, Val Acc: 90.11%

Epoch \[9/10\] Train Loss: 0.3232, Train Acc: 88.33% Val Loss: 0.3461, Val Acc: 89.01%

Epoch \[10/10\] Train Loss: 0.3592, Train Acc: 86.99% Val Loss: 0.3157, Val Acc: 86.81%

Metrics ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô:

- Accuracy ‚Üí ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™
- Confusion Matrix ‚Üí ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏ö‡∏™‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™
- Visualization ‚Üí Heatmap ‡∏Ç‡∏≠‡∏á Attention Module ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ô‡πâ‡∏ô‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡πÄ‡∏•‡πá‡∏ö‡∏à‡∏£‡∏¥‡∏á

‡πÇ‡∏°‡πÄ‡∏î‡∏• ResNet18 ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Spatial Attention ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÇ‡∏£‡∏Ñ‡πÄ‡∏•‡πá‡∏ö 6 ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô 10 Epochs ‡πÇ‡∏î‡∏¢‡∏°‡∏µ Validation Accuracy ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà 95.6% ‡πÉ‡∏ô Epoch ‡∏ó‡∏µ‡πà 5 ‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏•‡∏î‡∏•‡∏á‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡πÑ‡∏õ‡∏ó‡∏µ‡πà 86.81% ‡πÉ‡∏ô Epoch ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢

‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏Ñ‡∏∑‡∏≠ Overfitting: ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å Epoch ‡∏ó‡∏µ‡πà 5 ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏õ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Training ‡πÑ‡∏î‡πâ‡∏î‡∏µ (Training Loss ‡∏•‡∏î‡∏•‡∏á, Training Accuracy ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô) ‡πÅ‡∏ï‡πà‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏ö‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Validation ‡∏Å‡∏•‡∏±‡∏ö‡πÅ‡∏¢‡πà‡∏•‡∏á (Validation Loss ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô, Validation Accuracy ‡∏•‡∏î‡∏•‡∏á‡πÅ‡∏•‡∏∞‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô) ‡∏ã‡∏∂‡πà‡∏á‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏î‡∏à‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Training ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏à‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥

‡∏à‡∏≤‡∏Å Confusion Matrix ‡∏û‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏•‡∏≤‡∏™ Healthy_Nail, Acral_Lentiginous_Melanoma, pitting ‡πÅ‡∏•‡∏∞ blue_finger ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏°‡∏≤‡∏Å (‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏ñ‡∏∂‡∏á 100% ‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏•‡∏≤‡∏™) ‡πÅ‡∏ï‡πà‡∏°‡∏µ ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏Ñ‡∏•‡∏≤‡∏™ clubbing ‡πÅ‡∏•‡∏∞ Onychogryphosis ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ clubbing ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏¥‡∏î‡πÑ‡∏õ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏•‡∏≤‡∏™‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î

‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏Ñ‡∏∑‡∏≠: ‡∏Ñ‡∏ß‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ Early Stopping ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏ö‡∏ô‡∏ä‡∏∏‡∏î Validation ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏•‡∏î‡∏•‡∏á (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì Epoch ‡∏ó‡∏µ‡πà 5) ‡πÅ‡∏•‡∏∞‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏ß‡∏¥‡∏ò‡∏µ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ Regularization

**‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÅ‡∏•‡∏∞‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (References)**

‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏µ‡∏°‡∏≤‡∏ô‡∏µ‡πâ ‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡πÄ‡∏•‡πá‡∏ö (nail images) ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ deep learning ‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏•‡πá‡∏ö‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡∏ú‡∏¥‡∏ß‡∏´‡∏ô‡∏±‡∏á ‡∏†‡∏π‡∏°‡∏¥‡∏Ñ‡∏∏‡πâ‡∏°‡∏Å‡∏±‡∏ô ‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÉ‡∏ô‡∏™‡∏≤‡∏Ç‡∏≤‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏ö‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏´‡∏•‡∏±‡∏Å ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ

**1 ‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏î‡πâ‡∏≤‡∏ô‡πÄ‡∏•‡πá‡∏ö**

- Autonomous detection of nail disorders using a hybrid capsule CNN (Shandilya et al., 2024) ‡πÄ‡∏™‡∏ô‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏• Hybrid Capsule CNN ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÇ‡∏£‡∏Ñ‡πÄ‡∏•‡πá‡∏ö 6 ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó ‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (Nail Disease Detection Dataset) ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏• training accuracy ~99.40% ‡πÅ‡∏•‡∏∞ validation accuracy ~99.25% ‡∏ã‡∏∂‡πà‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á‡∏®‡∏±‡∏Å‡∏¢‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á deep learning ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÇ‡∏£‡∏Ñ‡πÄ‡∏•‡πá‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
- Nail Disease Detection and Classification Using Deep Learning (R. Regin et al., 2022) ‡πÉ‡∏ä‡πâ CNN ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ machine learning ‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏° (SVM, KNN, RF) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û‡πÄ‡∏•‡πá‡∏ö ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏µ‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡πá‡∏ö
- Assessment of Nail Images for Preliminary disease detection and classification based on CNN: The New Horizon in Disease Detection in Humans (Marulkar & Narain, 2022) ‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡∏†‡∏≤‡∏û‡πÄ‡∏•‡πá‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô ~750 ‡∏†‡∏≤‡∏û ‡πÅ‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡πá‡∏ö ‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå deep learning ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÉ‡∏ô domain ‡πÄ‡∏•‡πá‡∏ö
- Artificial Intelligence in the Diagnosis of Onychomycosis-Literature Review (‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô, 2024) ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á AI/Deep Learning ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡πÇ‡∏£‡∏Ñ‡πÄ‡∏•‡πá‡∏ö ‡πÄ‡∏ä‡πà‡∏ô onychomycosis, subungual melanoma ‡πÇ‡∏î‡∏¢‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN ‡∏≠‡∏¢‡πà‡∏≤‡∏á VGG ‡∏´‡∏£‡∏∑‡∏≠ ResNet ‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏á‡∏≤‡∏ô ‡πÅ‡∏•‡∏∞‡∏ä‡∏µ‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô ‡∏Ç‡∏≤‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏ç‡πà ‡πÅ‡∏•‡∏∞‡∏Ç‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° (explainability)

**2 ‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢ Attention Mechanism ‡πÅ‡∏•‡∏∞‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á**

- CA‚ÄëNet: Comprehensive Attention Convolutional Neural Networks for Explainable Medical Image Segmentation (Gu et al., 2020) ‡πÄ‡∏™‡∏ô‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á Spatial Attention, Channel Attention, ‡πÅ‡∏•‡∏∞ Scale Attention ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô segmentation ‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå
- Attention Based Glaucoma Detection: A Large‚Äëscale Database and CNN Model (Li et al., 2019) ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Spatial Attention module ‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û fundus ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à glaucoma ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡∏µ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≥ Attention ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå

**3 ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î**

‡∏à‡∏≤‡∏Å‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡πÄ‡∏•‡πá‡∏ö‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÉ‡∏ä‡πâ CNN ‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡∏¢‡∏±‡∏á ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡∏î‡∏π‡∏• Attention ‡∏ó‡∏≤‡∏á spatial ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÅ‡∏•‡∏∞‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏≤‡∏ô Attention ‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ Spatial (‡πÅ‡∏•‡∏∞ Channel) Attention ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• "‡πÇ‡∏ü‡∏Å‡∏±‡∏™" ‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô  
‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ ResNet-18 + Spatial Attention Module ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô domain "‡∏†‡∏≤‡∏û‡πÄ‡∏•‡πá‡∏ö" ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏á‡∏≤‡∏ô‡∏ô‡πâ‡∏≠‡∏¢ ‡πÅ‡∏•‡∏∞‡πÇ‡∏°‡∏î‡∏π‡∏• Attention ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡πÄ‡∏•‡πá‡∏ö (nail area) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
